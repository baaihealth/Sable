
_target_: sable.module.antibody_design_module.AntibodyDesignModule

loss:
  _target_: sable.loss.loss.AntibodyDesignLoss
  config:
    atom_level: ${atom_level}
    fape:
      sidechain:
        clamp_distance: 15.0
        length_scale: 10.0
        weight: 0.5
      backbone:
        clamp_distance: 15.0
        loss_unit_distance: 10.0
        weight: 0.5
      intra_chain_backbone:
        clamp_distance: 10.0
        loss_unit_distance: 10.0
        weight: 1.0
      interface_backbone:
        clamp_distance: 1000
        loss_unit_distance: 20.0
        weight: 1.0
      center_of_mass:
        weight: 0.0
      eps: 1e-4
      weight: 3.0

model:
  _target_: sable.model.model.AntibodyDesign
  config:
    cdr_mask: ${cdr_dict.${dataset}.cdr_mask}
    concat:
      activation_dropout: 0.0
      activation_fn: "gelu"
      attention_dropout: 0.1
      dropout: 0.1
      emb_dropout: 0.1
      encoder_attention_heads: 128
      encoder_embed_dim: 512
      encoder_ffn_embed_dim: 2048
      encoder_layers: 4
      post_ln: False
    sable:
      atom_level: ${atom_level}
      activation_dropout: 0.0
      activation_fn: "gelu"
      attention_dropout: 0.1
      dropout: 0.1
      emb_dropout: 0.1
      encoder_attention_heads: 128
      encoder_embed_dim: 512
      encoder_ffn_embed_dim: 2048
      encoder_layers: 15
      loss_distance_scalar: -1
      loss_residue_scalar: -1
      post_ln: False
    recycling: ${cdr_dict.${dataset}.recycling}
    structure_module:
      c_s: 384
      c_z: 256
      c_ipa: 16
      c_resnet: 128
      no_heads_ipa: 12
      no_qk_points: 4
      no_v_points: 8
      dropout_rate: 0.1
      no_blocks: 8
      no_transition_layers: 1
      no_resnet_blocks: 2
      no_angles: 7
      trans_scale_factor: 10
      epsilon: 1.0e-08
      'inf': 100000.0
      share_weights: true
      enable_chain_frame_update: false
      use_chain_emb: false

optimizer:
  _partial_: True
  _target_: torch.optim.Adam
  betas: [0.9, 0.99]
  eps: 1e-6

sable_config:
  atom_level: ${atom_level}
  loss_config:
    atom_level: ${atom_level}
    loss_distance_scalar: ${module.sable_config.model_config.loss_distance_scalar}
    loss_residue_scalar: ${module.sable_config.model_config.loss_residue_scalar}
  model_config:
    atom_level: ${atom_level}
    activation_dropout: 0.0
    activation_fn: "gelu"
    attention_dropout: 0.1
    dropout: 0.1
    emb_dropout: 0.1
    encoder_attention_heads: 128
    encoder_embed_dim: 512
    encoder_ffn_embed_dim: 2048
    encoder_layers: 15
    loss_distance_scalar: 1
    loss_residue_scalar: 1
    post_ln: False

scheduler:
  _partial_: True
  _target_: sable.optim.lr_scheduler.SableLRScheduler
  max_lr: 3e-4
  buffer: ${trainer.max_epochs}
  warmup_ratio: 0.06

